{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29409d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Before Dropping Sparse Columns ---\n",
      "Shape: 3613 rows × 46 cols\n",
      "                                           Missing_Count  Missing_Percent\n",
      "Reconstruction Costs ('000 US$)                     3606        99.806255\n",
      "Reconstruction Costs, Adjusted ('000 US$)           3606        99.806255\n",
      "AID Contribution ('000 US$)                         3522        97.481317\n",
      "River Basin                                         3194        88.402989\n",
      "No. Homeless                                        3146        87.074453\n",
      "Insured Damage, Adjusted ('000 US$)                 3098        85.745918\n",
      "Insured Damage ('000 US$)                           3095        85.662884\n",
      "Event Name                                          3013        83.393302\n",
      "Latitude                                            2944        81.483532\n",
      "Longitude                                           2944        81.483532\n",
      "External IDs                                        2481        68.668696\n",
      "No. Injured                                         2426        67.146416\n",
      "Associated Types                                    2360        65.319679\n",
      "Origin                                              2353        65.125934\n",
      "Magnitude                                           2213        61.251038\n",
      "Total Damage, Adjusted ('000 US$)                   1997        55.272627\n",
      "Total Damage ('000 US$)                             1989        55.051204\n",
      "No. Affected                                        1595        44.146139\n",
      "Total Affected                                       956        26.460006\n",
      "Total Deaths                                         846        23.415444\n",
      "Admin Units                                          491        13.589815\n",
      "Start Day                                            327         9.050650\n",
      "End Day                                              323         8.939939\n",
      "Magnitude Scale                                      212         5.867700\n",
      "Location                                              88         2.435649\n",
      "CPI                                                   57         1.577636\n",
      "End Month                                             27         0.747301\n",
      "Start Month                                            6         0.166067\n",
      "\n",
      "--- After Dropping Sparse Columns ---\n",
      "Shape: 3556 rows × 36 cols\n",
      "                                   Missing_Count  Missing_Percent\n",
      "External IDs                                2427        68.250844\n",
      "No. Injured                                 2399        67.463442\n",
      "Associated Types                            2333        65.607424\n",
      "Origin                                      2320        65.241845\n",
      "Magnitude                                   2158        60.686164\n",
      "Total Damage, Adjusted ('000 US$)           1940        54.555681\n",
      "Total Damage ('000 US$)                     1940        54.555681\n",
      "No. Affected                                1569        44.122610\n",
      "Total Affected                               943        26.518560\n",
      "Total Deaths                                 831        23.368954\n",
      "Admin Units                                  434        12.204724\n",
      "Start Day                                    325         9.139483\n",
      "End Day                                      321         9.026997\n",
      "Magnitude Scale                              206         5.793026\n",
      "Location                                      88         2.474691\n",
      "End Month                                     27         0.759280\n",
      "Start Month                                    6         0.168729\n",
      "[INFO] Fixing duplicates in Agriculture...\n",
      "[INFO] Fixing duplicates in Population...\n",
      "[INFO] Fixing duplicates in Disaster...\n",
      "✅ Individual cleaned datasets saved!\n",
      "✅ Final merged dataset saved: ../data_clean\\merged_country_year.csv\n",
      "✅ Shape: (521, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ===================================\n",
    "# Canonical Countries + Name Mapping\n",
    "# ===================================\n",
    "CANON_COUNTRIES = [\n",
    "    \"India\", \"USA\", \"Russia\", \"France\", \"Germany\", \"Italy\", \"China\", \"Japan\",\n",
    "    \"Argentina\", \"Portugal\", \"Spain\", \"Croatia\", \"Belgium\", \"Australia\",\n",
    "    \"Pakistan\", \"Afghanistan\", \"Israel\", \"Iran\", \"Iraq\", \"Bangladesh\",\n",
    "    \"Sri Lanka\", \"Canada\", \"UK\", \"Sweden\", \"Saudi Arabia\"\n",
    "]\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    \"United States\": \"USA\", \"United States of America\": \"USA\", \"U.S.\": \"USA\", \"US\": \"USA\",\n",
    "    \"Russian Federation\": \"Russia\",\n",
    "    \"United Kingdom\": \"UK\", \"Great Britain\": \"UK\", \"Britain\": \"UK\", \"U.K.\": \"UK\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\": \"UK\",\n",
    "    \"Iran, Islamic Rep.\": \"Iran\", \"Islamic Republic of Iran\": \"Iran\", \"Iran (Islamic Republic of)\": \"Iran\",\n",
    "    \"Republic of Iraq\": \"Iraq\",\n",
    "    \"Islamic Republic of Afghanistan\": \"Afghanistan\",\n",
    "    \"China, P.R.: Mainland\": \"China\", \"People's Republic of China\": \"China\", \"Mainland China\": \"China\",\n",
    "    \"Dem. Socialist Republic of Sri Lanka\": \"Sri Lanka\", \"SriLanka\": \"Sri Lanka\",\n",
    "    \"Saudi Arabia, Kingdom of\": \"Saudi Arabia\"\n",
    "}\n",
    "\n",
    "def standardize_country_names(df, col=\"Country\"):\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    df[col] = df[col].astype(str).str.strip().replace(COUNTRY_MAP)\n",
    "    return df[df[col].isin(CANON_COUNTRIES)].copy()\n",
    "\n",
    "def coerce_year_to_int64(series):\n",
    "    s = series.astype(str).str.extract(r'(\\d{4})', expand=False)\n",
    "    return pd.to_numeric(s, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "def clean_wide_metric_names(cols):\n",
    "    cleaned = []\n",
    "    for c in cols:\n",
    "        c = re.sub(r\"\\s+\", \"_\", str(c).strip())\n",
    "        c = c.replace(\"%\", \"pct\").replace(\"/\", \"_per_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        c = re.sub(r\"[^0-9a-zA-Z_]+\", \"_\", c)\n",
    "        c = re.sub(r\"_+\", \"_\", c).strip(\"_\")\n",
    "        cleaned.append(c)\n",
    "    return cleaned\n",
    "\n",
    "# ===================================\n",
    "# Dataset Preprocessors\n",
    "def drop_sparse_cols(df, min_non_missing=0.2): \n",
    "    \"\"\"\n",
    "    Drop columns with fewer than `min_non_missing` fraction of non-missing values.\n",
    "    Example: min_non_missing=0.3 → drop column if <30% non-missing values.\n",
    "    \"\"\"\n",
    "    thresh = int(len(df) * min_non_missing)\n",
    "    return df.dropna(axis=1, thresh=thresh)\n",
    "\n",
    "\n",
    "def preprocess_agriculture(filepath, missing_threshold=0.8):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Rename\n",
    "    df.rename(columns={\"Area\": \"Country\"}, inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Convert Year and Value to numeric\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\")\n",
    "\n",
    "    # Replace placeholders with NaN\n",
    "    df.replace([\"..\", \"\", \"NaN\"], np.nan, inplace=True)\n",
    "\n",
    "    # Fill missing numeric Value with 0\n",
    "    df[\"Value\"] = df[\"Value\"].fillna(0)\n",
    "\n",
    "    # Standardize country names\n",
    "    df = standardize_country_names(df, \"Country\")\n",
    "\n",
    "    # Filter years\n",
    "    df = df[(df[\"Year\"] >= 2000) & (df[\"Year\"] <= 2024)]\n",
    "\n",
    "    # Drop columns with >80% missing values (applies to numeric & categorical)\n",
    "    threshold_count = int(len(df) * missing_threshold)\n",
    "    df = df.dropna(axis=1, thresh=len(df) - threshold_count)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def summarize_missing(df, title=\"\"):\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} cols\")\n",
    "    missing_summary = (\n",
    "        df.isna().sum()\n",
    "        .to_frame(\"Missing_Count\")\n",
    "        .assign(Missing_Percent=lambda x: (x[\"Missing_Count\"] / len(df)) * 100)\n",
    "        .query(\"Missing_Count > 0\")\n",
    "        .sort_values(\"Missing_Percent\", ascending=False)\n",
    "    )\n",
    "    if missing_summary.empty:\n",
    "        print(\"No missing values ✅\")\n",
    "    else:\n",
    "        print(missing_summary)\n",
    "\n",
    "\n",
    "def preprocess_disasters(filepath, savepath=\"cleaned_dataset.csv\"):\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # --- Summary BEFORE dropping ---\n",
    "    summarize_missing(df, \"Before Dropping Sparse Columns\")\n",
    "\n",
    "    # --- Standardize Year column ---\n",
    "    if \"Start Year\" in df.columns:\n",
    "        df.rename(columns={\"Start Year\": \"Year\"}, inplace=True)\n",
    "    if \"Year\" in df.columns:\n",
    "        df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # --- Replace placeholder missing values with NaN ---\n",
    "    df.replace([\"..\", \"\", \"NaN\", \"nan\", \"NULL\", \"N/A\"], np.nan, inplace=True)\n",
    "\n",
    "    # --- Standardize country names ---\n",
    "    if \"Country\" in df.columns and \"standardize_country_names\" in globals():\n",
    "        df = standardize_country_names(df, \"Country\")\n",
    "\n",
    "    # --- Keep only years in valid range ---\n",
    "    if \"Year\" in df.columns:\n",
    "        df = df[(df[\"Year\"] >= 2000) & (df[\"Year\"] <= 2024)]\n",
    "\n",
    "    # --- Drop sparse cols (>70% missing) BEFORE imputing ---\n",
    "    thresh = len(df) * 0.2\n",
    "    df = df.dropna(axis=1, thresh=thresh)\n",
    "\n",
    "    # --- Summary AFTER dropping ---\n",
    "    summarize_missing(df, \"After Dropping Sparse Columns\")\n",
    "\n",
    "    # --- Handle remaining missing values ---\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > 0:\n",
    "            if df[col].dtype.kind in \"biufc\":  # numeric\n",
    "                df[col] = df[col].fillna(0)   # or mean/median\n",
    "            else:  # categorical\n",
    "                df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "    # --- Drop duplicates ---\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # --- Save ---\n",
    "    df.to_csv(savepath, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_population(filepath, missing_threshold=0.8):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Strip column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Rename\n",
    "    df.rename(columns={\"Area\": \"Country\"}, inplace=True)\n",
    "    \n",
    "    # Convert Year to numeric\n",
    "    if \"Year\" in df: \n",
    "        df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    \n",
    "    # Replace placeholders\n",
    "    df.replace([\"..\", \"\", \"NaN\"], np.nan, inplace=True)\n",
    "    \n",
    "    # Convert Value\n",
    "    if \"Unit\" in df and \"Value\" in df:\n",
    "        df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\")\n",
    "        mask = (df[\"Unit\"] == \"1000 No\") & df[\"Value\"].notna()\n",
    "        df.loc[mask, \"Value\"] *= 1000\n",
    "    \n",
    "    # Standardize country names\n",
    "    df = standardize_country_names(df, \"Country\")\n",
    "    \n",
    "    # Filter years\n",
    "    df = df[(df[\"Year\"] >= 2000) & (df[\"Year\"] <= 2024)]\n",
    "    \n",
    "    # Drop sparse columns (both numeric & categorical)\n",
    "    threshold_count = int(len(df) * missing_threshold)\n",
    "    df = df.dropna(axis=1, thresh=len(df) - threshold_count)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_resilience(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    id_cols = [c for c in [\"Country Name\", \"Country Code\", \"Series Name\", \"Series Code\"] if c in df]\n",
    "    year_cols = [c for c in df if re.search(r\"\\b(200\\d|201\\d|202[0-4])\\b\", str(c))]\n",
    "    df = df.dropna(subset=year_cols, how=\"all\")\n",
    "\n",
    "    df_long = df.melt(id_vars=id_cols, value_vars=year_cols,\n",
    "                      var_name=\"Year_raw\", value_name=\"Value\")\n",
    "    df_long[\"Year\"] = coerce_year_to_int64(df_long[\"Year_raw\"])\n",
    "    df_long.dropna(subset=[\"Year\"], inplace=True)\n",
    "    df_long[\"Value\"] = pd.to_numeric(df_long[\"Value\"], errors=\"coerce\")\n",
    "    df_long = df_long[df_long[\"Year\"].between(2000, 2024)]\n",
    "    if \"Country Name\" in df_long: df_long.rename(columns={\"Country Name\": \"Country\"}, inplace=True)\n",
    "    df_long = standardize_country_names(df_long, \"Country\")\n",
    "\n",
    "    # pivot → wide\n",
    "    df_pivot = df_long.pivot_table(index=[\"Country\", \"Year\"],\n",
    "                                   columns=\"Series Name\",\n",
    "                                   values=\"Value\",\n",
    "                                   aggfunc=\"first\")\n",
    "    df_pivot.columns = clean_wide_metric_names(df_pivot.columns)\n",
    "    df_pivot.reset_index(inplace=True)\n",
    "\n",
    "    df_pivot = drop_sparse_cols(df_pivot)\n",
    "    numeric_cols = df_pivot.select_dtypes(include=\"number\").columns\n",
    "    for col in numeric_cols:\n",
    "        df_pivot[col] = (df_pivot.sort_values([\"Country\", \"Year\"])\n",
    "                                  .groupby(\"Country\")[col]\n",
    "                                  .transform(lambda x: x.interpolate().ffill().bfill()))\n",
    "    df_pivot[numeric_cols] = df_pivot[numeric_cols].fillna(df_pivot[numeric_cols].median())\n",
    "    return df_pivot.drop_duplicates().sort_values([\"Country\", \"Year\"]).reset_index(drop=True)\n",
    "\n",
    "def make_unique(df, name):\n",
    "    if df.duplicated(subset=[\"Country\", \"Year\"]).any():\n",
    "        print(f\"[INFO] Fixing duplicates in {name}...\")\n",
    "        df = df.groupby([\"Country\", \"Year\"], as_index=False).mean(numeric_only=True)\n",
    "    return df\n",
    "\n",
    "# ===================================\n",
    "# Paths\n",
    "# ===================================\n",
    "paths = {\n",
    "    \"raw_dir\": \"../data_raw\",\n",
    "    \"clean_dir\": \"../data_clean\",\n",
    "}\n",
    "\n",
    "os.makedirs(paths[\"clean_dir\"], exist_ok=True)\n",
    "\n",
    "# ===================================\n",
    "# Run Preprocessing\n",
    "# ===================================\n",
    "agri_clean = preprocess_agriculture(os.path.join(paths[\"raw_dir\"], \"crop_and_livestock.csv\"))\n",
    "population_clean = preprocess_population(os.path.join(paths[\"raw_dir\"], \"population_and_demographics.csv\"))\n",
    "disaster_clean = preprocess_disasters(os.path.join(paths[\"raw_dir\"], \"disasters.csv\"))\n",
    "resilience_clean = preprocess_resilience(os.path.join(paths[\"raw_dir\"], \"Resiliance.csv\"))\n",
    "\n",
    "# Ensure uniqueness\n",
    "agri_clean = make_unique(agri_clean, \"Agriculture\")\n",
    "population_clean = make_unique(population_clean, \"Population\")\n",
    "disaster_clean = make_unique(disaster_clean, \"Disaster\")\n",
    "resilience_clean = make_unique(resilience_clean, \"Resilience\")\n",
    "\n",
    "# ===================================\n",
    "# Save each cleaned dataset\n",
    "# ===================================\n",
    "agri_clean.to_csv(os.path.join(paths[\"clean_dir\"], \"agriculture_clean.csv\"), index=False)\n",
    "population_clean.to_csv(os.path.join(paths[\"clean_dir\"], \"population_clean.csv\"), index=False)\n",
    "disaster_clean.to_csv(os.path.join(paths[\"clean_dir\"], \"disasters_clean.csv\"), index=False)\n",
    "resilience_clean.to_csv(os.path.join(paths[\"clean_dir\"], \"resilience_clean.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Individual cleaned datasets saved!\")\n",
    "\n",
    "# ===================================\n",
    "# Merge all datasets\n",
    "# ===================================\n",
    "merged_df = (\n",
    "    agri_clean\n",
    "    .merge(population_clean, on=[\"Country\", \"Year\"], how=\"outer\")\n",
    "    .merge(disaster_clean, on=[\"Country\", \"Year\"], how=\"outer\")\n",
    "    .merge(resilience_clean, on=[\"Country\", \"Year\"], how=\"outer\")\n",
    ")\n",
    "\n",
    "# Fill NaN in disaster-related columns\n",
    "disaster_cols = [c for c in merged_df if \"Disaster\" in c]\n",
    "merged_df[disaster_cols] = merged_df[disaster_cols].fillna(0)\n",
    "\n",
    "# Drop remaining nulls safely\n",
    "merged_df = merged_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save final merged dataset\n",
    "out_path = os.path.join(paths[\"clean_dir\"], \"merged_country_year.csv\")\n",
    "merged_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"✅ Final merged dataset saved:\", out_path)\n",
    "print(\"✅ Shape:\", merged_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
