{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c658ae-2ea0-481f-a278-0d428832bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame 1 - isAggregate values:\n",
      "Unique values: [False  True]\n",
      "Value counts:\n",
      " isAggregate\n",
      "False    33286\n",
      "True      8652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame 2 - isAggregate values:\n",
      "Unique values: [False  True]\n",
      "Value counts:\n",
      " isAggregate\n",
      "True     33520\n",
      "False     8356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame 3 - isAggregate values:\n",
      "Unique values: [False  True]\n",
      "Value counts:\n",
      " isAggregate\n",
      "False    38050\n",
      "True      9323\n",
      "Name: count, dtype: int64\n",
      "\n",
      "DataFrame 4 - isAggregate values:\n",
      "Unique values: [False  True]\n",
      "Value counts:\n",
      " isAggregate\n",
      "True     37066\n",
      "False     9245\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "BEGINNING DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "\n",
      "======================================== Processing Export DataFrame 1 ========================================\n",
      "\n",
      "Original missing values:\n",
      "qtyunitabbr       41938\n",
      "qty               33264\n",
      "altqtyunitabbr    41938\n",
      "altqty            33286\n",
      "netwgt            33689\n",
      "grosswgt          33615\n",
      "cifvalue          36723\n",
      "dtype: int64\n",
      "\n",
      "isaggregate column value counts:\n",
      "isaggregate\n",
      "False    33286\n",
      "True      8652\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dropping columns with 100% missing values: ['qtyunitabbr', 'altqtyunitabbr']\n",
      "Filling high-missing numeric qty with 0\n",
      "Filling high-missing numeric altqty with 0\n",
      "Filling high-missing numeric netwgt with 0\n",
      "Filling high-missing numeric grosswgt with 0\n",
      "Filling high-missing numeric cifvalue with 0\n",
      "\n",
      "Filling 0 missing cifvalue with fobvalue\n",
      "\n",
      "All missing values successfully handled\n",
      "\n",
      "Export DataFrame 1 shape after cleaning: (41938, 50)\n",
      "\n",
      "======================================== Processing Export DataFrame 2 ========================================\n",
      "\n",
      "Original missing values:\n",
      "qtyunitabbr       41876\n",
      "qty                8356\n",
      "altqtyunitabbr    41876\n",
      "altqty             8356\n",
      "netwgt            13503\n",
      "grosswgt          13223\n",
      "cifvalue          33134\n",
      "dtype: int64\n",
      "\n",
      "isaggregate column value counts:\n",
      "isaggregate\n",
      "True     33520\n",
      "False     8356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dropping columns with 100% missing values: ['qtyunitabbr', 'altqtyunitabbr']\n",
      "Filling high-missing numeric cifvalue with 0\n",
      "Filling moderate-missing numeric netwgt with median: 0.0\n",
      "Filling moderate-missing numeric grosswgt with median: 0.0\n",
      "Filling low-missing numeric qty with mean: 0.0\n",
      "Filling low-missing numeric altqty with mean: 0.0\n",
      "\n",
      "Filling 0 missing cifvalue with fobvalue\n",
      "\n",
      "All missing values successfully handled\n",
      "\n",
      "Export DataFrame 2 shape after cleaning: (41876, 50)\n",
      "\n",
      "======================================== Processing Import DataFrame 1 ========================================\n",
      "\n",
      "Original missing values:\n",
      "qtyunitabbr       47373\n",
      "qty               38028\n",
      "altqtyunitabbr    47373\n",
      "altqty            38050\n",
      "netwgt            38598\n",
      "grosswgt          38465\n",
      "cifvalue           2031\n",
      "fobvalue          39105\n",
      "dtype: int64\n",
      "\n",
      "isaggregate column value counts:\n",
      "isaggregate\n",
      "False    38050\n",
      "True      9323\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dropping columns with 100% missing values: ['qtyunitabbr', 'altqtyunitabbr']\n",
      "Filling high-missing numeric qty with 0\n",
      "Filling high-missing numeric altqty with 0\n",
      "Filling high-missing numeric netwgt with 0\n",
      "Filling high-missing numeric grosswgt with 0\n",
      "Filling high-missing numeric fobvalue with 0\n",
      "Filling low-missing numeric cifvalue with mean: 1659221911.5950615\n",
      "\n",
      "Filling 0 missing cifvalue with fobvalue\n",
      "\n",
      "All missing values successfully handled\n",
      "\n",
      "Import DataFrame 1 shape after cleaning: (47373, 51)\n",
      "\n",
      "======================================== Processing Import DataFrame 2 ========================================\n",
      "\n",
      "Original missing values:\n",
      "qtyunitabbr       46311\n",
      "qty                9245\n",
      "altqtyunitabbr    46311\n",
      "altqty             9245\n",
      "netwgt            15618\n",
      "grosswgt          14661\n",
      "cifvalue           1101\n",
      "fobvalue          31725\n",
      "dtype: int64\n",
      "\n",
      "isaggregate column value counts:\n",
      "isaggregate\n",
      "True     37066\n",
      "False     9245\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dropping columns with 100% missing values: ['qtyunitabbr', 'altqtyunitabbr']\n",
      "Filling moderate-missing numeric netwgt with median: 0.0\n",
      "Filling moderate-missing numeric grosswgt with median: 0.0\n",
      "Filling moderate-missing numeric fobvalue with median: 0.0\n",
      "Filling low-missing numeric qty with mean: 0.0\n",
      "Filling low-missing numeric altqty with mean: 0.0\n",
      "Filling low-missing numeric cifvalue with mean: 2754198405.926846\n",
      "\n",
      "Filling 0 missing cifvalue with fobvalue\n",
      "\n",
      "All missing values successfully handled\n",
      "\n",
      "Import DataFrame 2 shape after cleaning: (46311, 51)\n",
      "\n",
      "================================================================================\n",
      "PREPROCESSING COMPLETE - FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "\n",
      "DataFrame 1 Final Status:\n",
      "Shape: (41938, 50)\n",
      "Missing values: 0\n",
      "New columns added: ['qty_imputed', 'altqty_imputed', 'netwgt_imputed', 'grosswgt_imputed', 'cifvalue_imputed']\n",
      "First 3 rows:\n",
      "  typecode freqcode  refperiodid  refyear  refmonth  period  reportercode  \\\n",
      "0        C        A     20000101     2000        52    2000             8   \n",
      "1        C        A     20000101     2000        52    2000             8   \n",
      "2        C        A     20000101     2000        52    2000             8   \n",
      "\n",
      "  reporteriso reporterdesc flowcode  ...  fobvalue  primaryvalue  \\\n",
      "0         ALB      Albania        X  ...     209.0         209.0   \n",
      "1         ALB      Albania        X  ...   22288.0       22288.0   \n",
      "2         ALB      Albania        X  ...  266645.0      266645.0   \n",
      "\n",
      "  legacyestimationflag isreported  isaggregate qty_imputed altqty_imputed  \\\n",
      "0                    0      False        False           1              1   \n",
      "1                    0      False        False           1              1   \n",
      "2                    0      False        False           1              1   \n",
      "\n",
      "  netwgt_imputed grosswgt_imputed  cifvalue_imputed  \n",
      "0              1                1                 1  \n",
      "1              1                1                 1  \n",
      "2              1                1                 1  \n",
      "\n",
      "[3 rows x 50 columns]\n",
      "\n",
      "DataFrame 2 Final Status:\n",
      "Shape: (41876, 50)\n",
      "Missing values: 0\n",
      "New columns added: ['qty_imputed', 'altqty_imputed', 'netwgt_imputed', 'grosswgt_imputed', 'cifvalue_imputed']\n",
      "First 3 rows:\n",
      "  typecode freqcode  refperiodid  refyear  refmonth  period  reportercode  \\\n",
      "0        C        A     20130101     2013        52    2013             8   \n",
      "1        C        A     20130101     2013        52    2013            31   \n",
      "2        C        A     20130101     2013        52    2013            32   \n",
      "\n",
      "  reporteriso reporterdesc flowcode  ...     fobvalue  primaryvalue  \\\n",
      "0         ALB      Albania        X  ...        278.0         278.0   \n",
      "1         AZE   Azerbaijan        X  ...  205327392.0   205327392.0   \n",
      "2         ARG    Argentina        X  ...     165171.0      165171.0   \n",
      "\n",
      "  legacyestimationflag isreported  isaggregate qty_imputed altqty_imputed  \\\n",
      "0                    0       True        False           1              1   \n",
      "1                    0       True        False           1              1   \n",
      "2                    0       True        False           1              1   \n",
      "\n",
      "  netwgt_imputed grosswgt_imputed  cifvalue_imputed  \n",
      "0              1                1                 1  \n",
      "1              1                1                 1  \n",
      "2              1                1                 1  \n",
      "\n",
      "[3 rows x 50 columns]\n",
      "\n",
      "DataFrame 3 Final Status:\n",
      "Shape: (47373, 51)\n",
      "Missing values: 0\n",
      "New columns added: ['qty_imputed', 'altqty_imputed', 'netwgt_imputed', 'grosswgt_imputed', 'cifvalue_imputed', 'fobvalue_imputed']\n",
      "First 3 rows:\n",
      "  typecode freqcode  refperiodid  refyear  refmonth  period  reportercode  \\\n",
      "0        C        A     20000101     2000        52    2000             8   \n",
      "1        C        A     20000101     2000        52    2000             8   \n",
      "2        C        A     20000101     2000        52    2000             8   \n",
      "\n",
      "  reporteriso reporterdesc flowcode  ... primaryvalue  legacyestimationflag  \\\n",
      "0         ALB      Albania        M  ...     162552.0                     0   \n",
      "1         ALB      Albania        M  ...     776949.0                     0   \n",
      "2         ALB      Albania        M  ...    2936268.0                     0   \n",
      "\n",
      "  isreported isaggregate  qty_imputed altqty_imputed netwgt_imputed  \\\n",
      "0      False       False            1              1              1   \n",
      "1      False       False            1              1              1   \n",
      "2      False       False            1              1              1   \n",
      "\n",
      "  grosswgt_imputed cifvalue_imputed  fobvalue_imputed  \n",
      "0                1                0                 1  \n",
      "1                1                0                 1  \n",
      "2                1                0                 1  \n",
      "\n",
      "[3 rows x 51 columns]\n",
      "\n",
      "DataFrame 4 Final Status:\n",
      "Shape: (46311, 51)\n",
      "Missing values: 0\n",
      "New columns added: ['qty_imputed', 'altqty_imputed', 'netwgt_imputed', 'grosswgt_imputed', 'cifvalue_imputed', 'fobvalue_imputed']\n",
      "First 3 rows:\n",
      "  typecode freqcode  refperiodid  refyear  refmonth  period  reportercode  \\\n",
      "0        C        A     20130101     2013        52    2013             8   \n",
      "1        C        A     20130101     2013        52    2013            12   \n",
      "2        C        A     20130101     2013        52    2013            20   \n",
      "\n",
      "  reporteriso reporterdesc flowcode  ... primaryvalue  legacyestimationflag  \\\n",
      "0         ALB      Albania        M  ...     4751.000                     0   \n",
      "1         DZA      Algeria        M  ...      181.000                     0   \n",
      "2         AND      Andorra        M  ...     9160.033                     0   \n",
      "\n",
      "  isreported isaggregate  qty_imputed altqty_imputed netwgt_imputed  \\\n",
      "0       True       False            1              1              1   \n",
      "1       True       False            1              1              1   \n",
      "2      False        True            0              0              0   \n",
      "\n",
      "  grosswgt_imputed cifvalue_imputed  fobvalue_imputed  \n",
      "0                1                0                 1  \n",
      "1                1                0                 1  \n",
      "2                0                0                 1  \n",
      "\n",
      "[3 rows x 51 columns]\n",
      "Dropping completely empty columns: ['qtyunitabbr', 'altqtyunitabbr']\n",
      "Filled missing values in qty\n",
      "Filled missing values in altqty\n",
      "Filled missing values in netwgt\n",
      "Filled missing values in grosswgt\n",
      "Filled missing values in cifvalue\n",
      "Filled missing values in fobvalue\n",
      "Final data shape: (177498, 46)\n",
      "Missing values:\n",
      " typecode                    0\n",
      "freqcode                    0\n",
      "refperiodid                 0\n",
      "refyear                     0\n",
      "refmonth                    0\n",
      "period                      0\n",
      "reportercode                0\n",
      "reporteriso                 0\n",
      "reporterdesc                0\n",
      "flowcode                    0\n",
      "flowdesc                    0\n",
      "partnercode                 0\n",
      "partneriso                  0\n",
      "partnerdesc                 0\n",
      "partner2code                0\n",
      "partner2iso                 0\n",
      "partner2desc                0\n",
      "classificationcode          0\n",
      "classificationsearchcode    0\n",
      "isoriginalclassification    0\n",
      "cmdcode                     0\n",
      "cmddesc                     0\n",
      "aggrlevel                   0\n",
      "isleaf                      0\n",
      "customscode                 0\n",
      "customsdesc                 0\n",
      "moscode                     0\n",
      "motcode                     0\n",
      "motdesc                     0\n",
      "qtyunitcode                 0\n",
      "qty                         0\n",
      "isqtyestimated              0\n",
      "altqtyunitcode              0\n",
      "altqty                      0\n",
      "isaltqtyestimated           0\n",
      "netwgt                      0\n",
      "isnetwgtestimated           0\n",
      "grosswgt                    0\n",
      "isgrosswgtestimated         0\n",
      "cifvalue                    0\n",
      "fobvalue                    0\n",
      "primaryvalue                0\n",
      "legacyestimationflag        0\n",
      "isreported                  0\n",
      "isaggregate                 0\n",
      "trade_type                  0\n",
      "dtype: int64\n",
      "Year range: 2000 - 2024\n",
      "Unique reporters: 202\n",
      "Unique partners: 25\n",
      "\n",
      "Verifying the saved file by reading first 3 rows:\n",
      "  typecode freqcode refperiodid  refyear  refmonth  period  reportercode  \\\n",
      "0        C        A  2000-01-01     2000        52    2000             8   \n",
      "1        C        A  2000-01-01     2000        52    2000             8   \n",
      "2        C        A  2000-01-01     2000        52    2000             8   \n",
      "\n",
      "  reporteriso reporterdesc flowcode  ... isnetwgtestimated  grosswgt  \\\n",
      "0         ALB      Albania        X  ...             False       0.0   \n",
      "1         ALB      Albania        X  ...             False       0.0   \n",
      "2         ALB      Albania        X  ...             False       0.0   \n",
      "\n",
      "  isgrosswgtestimated  cifvalue  fobvalue primaryvalue legacyestimationflag  \\\n",
      "0               False     209.0     209.0        209.0                    0   \n",
      "1               False   22288.0   22288.0      22288.0                    0   \n",
      "2               False  266645.0  266645.0     266645.0                    0   \n",
      "\n",
      "  isreported isaggregate  trade_type  \n",
      "0      False       False      export  \n",
      "1      False       False      export  \n",
      "2      False       False      export  \n",
      "\n",
      "[3 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "data_dir = Path(\"~/OneDrive/Desktop/sem 5/Hackathon/DPL_25/Trade-Resilience-and-Economic-Networks/data_raw\").expanduser()\n",
    "\n",
    "# First define the file lists\n",
    "export_files = [\n",
    "    data_dir / \"2000-2012_Export.csv\",\n",
    "    data_dir / \"2013-2024_Export.csv\"\n",
    "]\n",
    "\n",
    "import_files = [\n",
    "    data_dir / \"2000-2012_Import.csv\",\n",
    "    data_dir / \"2013-2024_Import.csv\"\n",
    "]\n",
    "\n",
    "# Then use them in your loops\n",
    "export_dfs = []\n",
    "for file in export_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding='latin1')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file, encoding='windows-1252')\n",
    "    export_dfs.append(df)\n",
    "\n",
    "import_dfs = []\n",
    "for file in import_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding='latin1')\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(file, encoding='windows-1252')\n",
    "    import_dfs.append(df)\n",
    "\n",
    "# Rest of your code...\n",
    "# Verify the isAggregate column in all dataframes\n",
    "for i, df in enumerate(export_dfs + import_dfs):\n",
    "    print(f\"\\nDataFrame {i+1} - isAggregate values:\")\n",
    "    if 'isAggregate' in df.columns:\n",
    "        print(\"Unique values:\", df['isAggregate'].unique())\n",
    "        print(\"Value counts:\\n\", df['isAggregate'].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\"No isAggregate column in this dataframe\")\n",
    "\n",
    "# If any NA values remain, fill them with False\n",
    "for df in export_dfs + import_dfs:\n",
    "    if 'isAggregate' in df.columns:\n",
    "        na_count = df['isAggregate'].isna().sum()\n",
    "        if na_count > 0:\n",
    "            print(f\"\\nFilling {na_count} NA values in isAggregate with False\")\n",
    "            df['isAggregate'] = df['isAggregate'].fillna(False)\n",
    "            print(\"Updated value counts:\\n\", df['isAggregate'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "# Standardize column names\n",
    "def standardize_columns(df):\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    return df\n",
    "\n",
    "export_dfs = [standardize_columns(df) for df in export_dfs]\n",
    "import_dfs = [standardize_columns(df) for df in import_dfs]\n",
    "\n",
    "# Missing value preprocessing function\n",
    "def preprocess_trade_data(df):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Initial diagnostics\n",
    "    print(\"\\nOriginal missing values:\")\n",
    "    print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])\n",
    "    \n",
    "    # 2. Special check for isaggregate column\n",
    "    if 'isaggregate' in df_clean.columns:\n",
    "        print(\"\\nisaggregate column value counts:\")\n",
    "        print(df_clean['isaggregate'].value_counts(dropna=False))\n",
    "        \n",
    "        # Handle any potential missing values in isaggregate (if they exist)\n",
    "        if df_clean['isaggregate'].isnull().any():\n",
    "            print(f\"\\nFilling {df_clean['isaggregate'].isnull().sum()} missing values in isaggregate with FALSE\")\n",
    "            df_clean['isaggregate'] = df_clean['isaggregate'].fillna(False)\n",
    "    \n",
    "    # 2. Drop columns with 100% missing values\n",
    "    cols_to_drop = [col for col in df_clean.columns if df_clean[col].isnull().all()]\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nDropping columns with 100% missing values: {cols_to_drop}\")\n",
    "        df_clean.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # 3. Handle high missingness columns (70-100%)\n",
    "    high_missing_cols = [col for col in df_clean.columns \n",
    "                        if df_clean[col].isnull().mean() > 0.7 \n",
    "                        and col not in cols_to_drop]\n",
    "    \n",
    "    for col in high_missing_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df_clean[col]):\n",
    "            print(f\"Filling high-missing numeric {col} with 0\")\n",
    "            df_clean[col] = df_clean[col].fillna(0)\n",
    "        else:\n",
    "            print(f\"Filling high-missing categorical {col} with 'Unknown'\")\n",
    "            df_clean[col] = df_clean[col].fillna(\"Unknown\")\n",
    "    \n",
    "    # 4. Handle moderate missingness (20-70%)\n",
    "    moderate_missing_cols = [col for col in df_clean.columns \n",
    "                           if 0.2 < df_clean[col].isnull().mean() <= 0.7]\n",
    "    \n",
    "    for col in moderate_missing_cols:\n",
    "        if pd.api.types.is_numeric_dtype(df_clean[col]):\n",
    "            median_val = df_clean[col].median()\n",
    "            print(f\"Filling moderate-missing numeric {col} with median: {median_val}\")\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "        else:\n",
    "            mode_val = df_clean[col].mode()[0]\n",
    "            print(f\"Filling moderate-missing categorical {col} with mode: {mode_val}\")\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "    \n",
    "    # 5. Handle low missingness (<20%)\n",
    "    low_missing_cols = [col for col in df_clean.columns \n",
    "                       if 0 < df_clean[col].isnull().mean() <= 0.2]\n",
    "    \n",
    "    for col in low_missing_cols:\n",
    "        if col in ['refyear', 'refmonth', 'period']:\n",
    "            print(f\"Forward filling time-related {col}\")\n",
    "            df_clean[col] = df_clean[col].ffill()\n",
    "        elif pd.api.types.is_numeric_dtype(df_clean[col]):\n",
    "            mean_val = df_clean[col].mean()\n",
    "            print(f\"Filling low-missing numeric {col} with mean: {mean_val}\")\n",
    "            df_clean[col] = df_clean[col].fillna(mean_val)\n",
    "        else:\n",
    "            mode_val = df_clean[col].mode()[0]\n",
    "            print(f\"Filling low-missing categorical {col} with mode: {mode_val}\")\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "    \n",
    "    # 6. Special treatment for trade value columns\n",
    "    if 'cifvalue' in df_clean.columns and 'fobvalue' in df_clean.columns:\n",
    "        cif_missing = df_clean['cifvalue'].isnull()\n",
    "        print(f\"\\nFilling {cif_missing.sum()} missing cifvalue with fobvalue\")\n",
    "        df_clean.loc[cif_missing, 'cifvalue'] = df_clean.loc[cif_missing, 'fobvalue']\n",
    "    \n",
    "    # 7. Add imputation flags\n",
    "    for col in df_clean.columns:\n",
    "        if df[col].isnull().any():  # If there were originally missing values\n",
    "            flag_col = f\"{col}_imputed\"\n",
    "            df_clean[flag_col] = df[col].isnull().astype(int)\n",
    "    \n",
    "    # 8. Final check\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    if remaining_missing > 0:\n",
    "        print(f\"\\nWarning: {remaining_missing} missing values remain after preprocessing\")\n",
    "        print(df_clean.isnull().sum()[df_clean.isnull().sum() > 0])\n",
    "    else:\n",
    "        print(\"\\nAll missing values successfully handled\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Process all DataFrames with detailed logging\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BEGINNING DATA PREPROCESSING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "export_dfs_clean = []\n",
    "for i, df in enumerate(export_dfs):\n",
    "    print(f\"\\n{'='*40} Processing Export DataFrame {i+1} {'='*40}\")\n",
    "    df_clean = preprocess_trade_data(df)\n",
    "    export_dfs_clean.append(df_clean)\n",
    "    print(f\"\\nExport DataFrame {i+1} shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "import_dfs_clean = []\n",
    "for i, df in enumerate(import_dfs):\n",
    "    print(f\"\\n{'='*40} Processing Import DataFrame {i+1} {'='*40}\")\n",
    "    df_clean = preprocess_trade_data(df)\n",
    "    import_dfs_clean.append(df_clean)\n",
    "    print(f\"\\nImport DataFrame {i+1} shape after cleaning: {df_clean.shape}\")\n",
    "\n",
    "# Final validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "all_dfs = export_dfs_clean + import_dfs_clean\n",
    "for i, df in enumerate(all_dfs):\n",
    "    print(f\"\\nDataFrame {i+1} Final Status:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Missing values:\", df.isnull().sum().sum())\n",
    "    print(f\"New columns added: {[col for col in df.columns if '_imputed' in col]}\")\n",
    "    print(f\"First 3 rows:\\n{df.head(3)}\")\n",
    "    \n",
    "def convert_data_types(df):\n",
    "    # Convert date-related columns\n",
    "    if 'refperiodid' in df.columns:\n",
    "        df['refperiodid'] = pd.to_datetime(df['refperiodid'], format='%Y%m%d', errors='coerce')\n",
    "    \n",
    "    # Convert year to integer\n",
    "    if 'refyear' in df.columns:\n",
    "        df['refyear'] = pd.to_numeric(df['refyear'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Convert boolean columns\n",
    "    bool_cols = ['isoriginalclassification', 'isleaf', 'isqtyestimated', \n",
    "                 'isaltqtyestimated', 'isnetwgtestimated', 'isgrosswgtestimated',\n",
    "                 'isreported', 'isaggregate']\n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(bool)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "export_dfs = [convert_data_types(df) for df in export_dfs]\n",
    "import_dfs = [convert_data_types(df) for df in import_dfs]\n",
    "\n",
    "\n",
    "def clean_specific_columns(df):\n",
    "    # Clean reporter and partner descriptions\n",
    "    if 'reporterdesc' in df.columns:\n",
    "        df['reporterdesc'] = df['reporterdesc'].str.strip().str.title()\n",
    "    if 'partnerdesc' in df.columns:\n",
    "        df['partnerdesc'] = df['partnerdesc'].str.strip().str.title()\n",
    "    \n",
    "    # Ensure flow codes are consistent\n",
    "    if 'flowcode' in df.columns:\n",
    "        df['flowcode'] = df['flowcode'].str.upper().str.strip()\n",
    "    \n",
    "    # Clean commodity descriptions\n",
    "    if 'cmddesc' in df.columns:\n",
    "        df['cmddesc'] = df['cmddesc'].str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "export_dfs = [clean_specific_columns(df) for df in export_dfs]\n",
    "import_dfs = [clean_specific_columns(df) for df in import_dfs]\n",
    "\n",
    "# Concatenate export files\n",
    "exports = pd.concat(export_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# Concatenate import files\n",
    "imports = pd.concat(import_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "exports['trade_type'] = 'export'\n",
    "imports['trade_type'] = 'import'\n",
    "\n",
    "trade_data = pd.concat([exports, imports], axis=0, ignore_index=True)\n",
    "\n",
    "def final_cleanup(df):\n",
    "    \"\"\"Handle all remaining missing values after concatenation\"\"\"\n",
    "    # Input validation\n",
    "    if df is None:\n",
    "        raise ValueError(\"Input DataFrame is None\")\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "    if df.empty:\n",
    "        print(\"Warning: Empty DataFrame received\")\n",
    "        return df\n",
    "\n",
    "    # 1. Drop completely empty columns\n",
    "    cols_to_drop = [col for col in df.columns if df[col].isnull().all()]\n",
    "    if cols_to_drop:\n",
    "        print(f\"Dropping completely empty columns: {cols_to_drop}\")\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # 2. Fill rules dictionary (without trailing comma)\n",
    "    fill_rules = {\n",
    "        'qty': 0,\n",
    "        'altqty': 0,\n",
    "        'netwgt': 0,\n",
    "        'grosswgt': 0,\n",
    "        'cifvalue': lambda x: x['fobvalue'] if pd.notna(x['fobvalue']) else 0,\n",
    "        'fobvalue': 0,\n",
    "        'primaryvalue': lambda x: x['fobvalue'] if x.get('flowcode') == 'X' else x.get('cifvalue', 0),\n",
    "        'reporteriso': 'XX',\n",
    "        'partneriso': 'XX',\n",
    "        'partner2iso': 'XX',\n",
    "        'cmdcode': 'TOTAL',\n",
    "        'flowcode': lambda x: ('X' if 'export' in str(x.get('flowdesc', '')).lower() \n",
    "                             else ('M' if 'import' in str(x.get('flowdesc', '')).lower() \n",
    "                             else 'X')),\n",
    "        'isaggregate': False,\n",
    "        'isreported': False\n",
    "    }\n",
    "\n",
    "    # 3. Apply fill rules\n",
    "    for col, fill_value in fill_rules.items():\n",
    "        if col in df.columns and df[col].isnull().any():\n",
    "            try:\n",
    "                if callable(fill_value):\n",
    "                    df.loc[df[col].isnull(), col] = df[df[col].isnull()].apply(fill_value, axis=1)\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(fill_value)\n",
    "                print(f\"Filled missing values in {col}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error filling {col}: {str(e)}\")\n",
    "\n",
    "    # 4. Final cleanup\n",
    "    if 'flowcode' in df.columns:\n",
    "        df['flowcode'] = df['flowcode'].str.upper().str.strip().replace({'': 'X'})\n",
    "    \n",
    "    numeric_cols = ['qty', 'altqty', 'netwgt', 'grosswgt', 'cifvalue', 'fobvalue', 'primaryvalue']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply to the final combined DataFrame\n",
    "trade_data = final_cleanup(trade_data)\n",
    "\n",
    "# Check for consistency in combined data\n",
    "print(\"Final data shape:\", trade_data.shape)\n",
    "print(\"Missing values:\\n\", trade_data.isnull().sum())\n",
    "\n",
    "# Check time period coverage\n",
    "print(\"Year range:\", trade_data['refyear'].min(), \"-\", trade_data['refyear'].max())\n",
    "\n",
    "# Check unique countries\n",
    "print(\"Unique reporters:\", trade_data['reporteriso'].nunique())\n",
    "print(\"Unique partners:\", trade_data['partneriso'].nunique())\n",
    "\n",
    "# Define the output directory (same as input directory for consistency)\n",
    "output_dir = data_dir.parent / \"data_processed\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"~/OneDrive/Desktop/sem 5/Hackathon/DPL_25/Trade-Resilience-and-Economic-Networks/data_clean/trade_clean.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "trade_data.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "# Optional: Verify the saved file by reading a sample\n",
    "print(\"\\nVerifying the saved file by reading first 3 rows:\")\n",
    "print(pd.read_csv(output_file, nrows=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99766449-5d3c-4171-ab95-93cffa28ae5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\sreea\\OneDrive\\Desktop\\sem 5\\Hackathon\\DPL_25\\Trade-Resilience-and-Economic-Networks\\notebooks\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69de7c4-1457-4c98-b65b-55305748a847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
